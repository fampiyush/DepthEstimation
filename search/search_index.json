{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Depthlib","text":"<p>Depthlib is a Python library for depth estimation using stereo vision and monocular deep learning models.</p> <p>The library provides simple, high-level APIs that abstract away internal image processing and stereo matching details, making it easy to estimate depth from images and video streams.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Stereo Depth Estimation - Compute depth maps from rectified stereo image pairs using Semi-Global Block Matching (SGBM)</li> <li>Live Video Depth - Real-time depth estimation from synchronized stereo video streams or cameras</li> <li>Monocular Depth - Single-image depth estimation using pre-trained deep learning models (Depth Anything V2)</li> <li>Configurable Pipeline - Fine-tune SGBM parameters for optimal results in your scene</li> <li>Visualization Tools - Built-in functions for displaying disparity and depth maps</li> </ul>"},{"location":"#supported-modes","title":"Supported Modes","text":"Mode Class Description Stereo Images <code>StereoDepthEstimator</code> Depth from a single pair of stereo images Stereo Video <code>StereoDepthEstimatorVideo</code> Real-time depth from video streams/cameras Monocular <code>MonocularDepthEstimator</code> Depth from a single RGB image using AI"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install depthlib\n</code></pre>"},{"location":"#development-setup","title":"Development Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/your-username/depthlib.git\n\n# Create virtual environment\npython -m venv venv\n\n# Activate (Windows)\nvenv\\Scripts\\activate\n\n# Activate (macOS/Linux)\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import depthlib\n\n# Stereo depth estimation\nestimator = depthlib.StereoDepthEstimator(\n    left_source='left.png',\n    right_source='right.png',\n    downscale_factor=0.5\n)\nestimator.configure_sgbm(\n    num_disp=128,\n    focal_length=3997.684,\n    baseline=0.193\n)\ndisparity, depth = estimator.estimate_depth()\nestimator.visualize_results()\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>OpenCV</li> <li>NumPy</li> <li>Matplotlib</li> <li>PyTorch (for monocular depth)</li> <li>Transformers (for monocular depth)</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started - Setup guide and first examples</li> <li>Stereo Depth (Images) - Full API for image-based stereo depth</li> <li>Stereo Depth (Video) - Real-time video depth estimation</li> <li>SGBM Configuration - Tune stereo matching parameters</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>This page provides complete code examples for common use cases.</p>"},{"location":"examples/#stereo-image-depth-estimation","title":"Stereo Image Depth Estimation","text":""},{"location":"examples/#basic-example","title":"Basic Example","text":"<pre><code>import depthlib\n\n# Load stereo pair and estimate depth\nestimator = depthlib.StereoDepthEstimator(\n    left_source='./assets/stereo_pairs/im0.png',\n    right_source='./assets/stereo_pairs/im1.png',\n    downscale_factor=0.5\n)\n\n# Configure with camera parameters\nestimator.configure_sgbm(\n    num_disp=280,\n    focal_length=3997.684,\n    baseline=0.193,  # meters\n    doffs=131.111\n)\n\n# Estimate and visualize\ndisparity_px, depth_m = estimator.estimate_depth()\nestimator.visualize_results()\n</code></pre>"},{"location":"examples/#with-statistics","title":"With Statistics","text":"<pre><code>import depthlib\nimport numpy as np\nimport time\n\n# Setup\nestimator = depthlib.StereoDepthEstimator(\n    left_source='./left.png',\n    right_source='./right.png',\n    downscale_factor=0.5\n)\n\nestimator.configure_sgbm(\n    num_disp=128,\n    focal_length=679.01,\n    baseline=0.5725\n)\n\n# Measure timing\nstart = time.time()\ndisparity_px, depth_m = estimator.estimate_depth()\nelapsed_ms = (time.time() - start) * 1000\n\n# Print statistics\nprint(f\"Processing time: {elapsed_ms:.2f} ms\")\n\n# Disparity stats\nvalid_disp = disparity_px &gt; 0\nprint(f\"\\n=== Disparity ===\")\nprint(f\"Range: {disparity_px[valid_disp].min():.1f} - {disparity_px[valid_disp].max():.1f} px\")\nprint(f\"Invalid pixels: {(~valid_disp).sum() / valid_disp.size * 100:.1f}%\")\n\n# Depth stats\nif depth_m is not None:\n    valid_depth = np.isfinite(depth_m) &amp; (depth_m &gt; 0)\n    print(f\"\\n=== Depth ===\")\n    print(f\"Range: {depth_m[valid_depth].min():.2f} - {depth_m[valid_depth].max():.2f} m\")\n    print(f\"Mean: {depth_m[valid_depth].mean():.2f} m\")\n\nestimator.visualize_results()\n</code></pre>"},{"location":"examples/#live-video-depth-estimation","title":"Live Video Depth Estimation","text":""},{"location":"examples/#usb-cameras","title":"USB Cameras","text":"<pre><code>from depthlib import StereoDepthEstimatorVideo\n\n# Two USB cameras (indices 0 and 1)\nestimator = StereoDepthEstimatorVideo(\n    left_source=0,\n    right_source=1,\n    downscale_factor=0.5,\n    visualize_live=True,\n    fast_mode=True,      # Faster processing\n    drop_frames=True,    # Don't queue old frames\n    target_fps=30\n)\n\nestimator.configure_sgbm(\n    num_disp=128,\n    focal_length=679.01,\n    baseline=0.5725,\n    hole_filling=True\n)\n\nprint(\"Press ESC to exit\")\nfor depth_m in estimator.estimate_depth():\n    pass\n</code></pre>"},{"location":"examples/#video-files","title":"Video Files","text":"<pre><code>from depthlib import StereoDepthEstimatorVideo\n\nestimator = StereoDepthEstimatorVideo(\n    left_source='./assets/left.mp4',\n    right_source='./assets/right.mp4',\n    downscale_factor=0.7,\n    visualize_live=True,\n    drop_frames=False,   # Process all frames\n    target_fps=30\n)\n\nestimator.configure_sgbm(\n    num_disp=128,\n    focal_length=679.01,\n    baseline=0.5725\n)\n\nframe_count = 0\nfor depth_m in estimator.estimate_depth():\n    frame_count += 1\n\nprint(f\"Processed {frame_count} frames\")\n</code></pre>"},{"location":"examples/#save-closest-distance","title":"Save Closest Distance","text":"<pre><code>from depthlib import StereoDepthEstimatorVideo\nimport numpy as np\n\nestimator = StereoDepthEstimatorVideo(\n    left_source='./left.mp4',\n    right_source='./right.mp4',\n    visualize_live=True\n)\n\nestimator.configure_sgbm(\n    num_disp=128,\n    focal_length=679.01,\n    baseline=0.5725\n)\n\nclosest_distances = []\n\nfor depth_m in estimator.estimate_depth():\n    valid = np.isfinite(depth_m) &amp; (depth_m &gt; 0)\n    if valid.any():\n        closest = depth_m[valid].min()\n        closest_distances.append(closest)\n        print(f\"Closest object: {closest:.2f} m\")\n\n# Analyze results\nif closest_distances:\n    print(f\"\\n=== Summary ===\")\n    print(f\"Min distance seen: {min(closest_distances):.2f} m\")\n    print(f\"Max distance seen: {max(closest_distances):.2f} m\")\n</code></pre>"},{"location":"examples/#monocular-depth-estimation","title":"Monocular Depth Estimation","text":""},{"location":"examples/#basic-example_1","title":"Basic Example","text":"<pre><code>import depthlib\n\nmodel_path = \"models/hub/models--depth-anything--Depth-Anything-V2-Base-hf/snapshots/b1958afc87fb45a9e3746cb387596094de553ed8\"\n\nestimator = depthlib.MonocularDepthEstimator(\n    model_path=model_path,\n    device='cuda',\n    downscale_factor=0.5\n)\n\ndepth_map = estimator.estimate_depth('./image.png')\nestimator.visualize_depth()\n</code></pre>"},{"location":"examples/#batch-processing","title":"Batch Processing","text":"<pre><code>import depthlib\nimport os\nimport time\n\nmodel_path = \"path/to/model\"\nimage_dir = \"./images\"\noutput_dir = \"./depth_maps\"\n\nos.makedirs(output_dir, exist_ok=True)\n\n# Initialize once\nestimator = depthlib.MonocularDepthEstimator(\n    model_path=model_path,\n    device='cuda',\n    downscale_factor=1.0\n)\n\n# Process all images\nimage_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg'))]\n\nfor filename in image_files:\n    image_path = os.path.join(image_dir, filename)\n\n    start = time.time()\n    depth_map = estimator.estimate_depth(image_path)\n    elapsed = (time.time() - start) * 1000\n\n    print(f\"{filename}: {elapsed:.1f} ms\")\n\n    # Save depth map\n    import numpy as np\n    output_path = os.path.join(output_dir, f\"depth_{filename}\")\n    np.save(output_path.replace('.png', '.npy').replace('.jpg', '.npy'), depth_map)\n</code></pre>"},{"location":"examples/#custom-visualization","title":"Custom Visualization","text":""},{"location":"examples/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"<pre><code>import depthlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Estimate depth\nestimator = depthlib.StereoDepthEstimator(\n    left_source='./left.png',\n    right_source='./right.png'\n)\nestimator.configure_sgbm(num_disp=128, focal_length=1000, baseline=0.1)\ndisparity_px, depth_m = estimator.estimate_depth()\n\n# Custom visualization\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original image\nimport cv2\nleft = cv2.imread('./left.png')\nleft_rgb = cv2.cvtColor(left, cv2.COLOR_BGR2RGB)\naxes[0].imshow(left_rgb)\naxes[0].set_title('Left Image')\naxes[0].axis('off')\n\n# Disparity\nvalid_disp = disparity_px &gt; 0\nim1 = axes[1].imshow(disparity_px, cmap='jet', \n                      vmin=np.percentile(disparity_px[valid_disp], 1),\n                      vmax=np.percentile(disparity_px[valid_disp], 99))\naxes[1].set_title('Disparity (pixels)')\naxes[1].axis('off')\nplt.colorbar(im1, ax=axes[1], fraction=0.046)\n\n# Depth\nif depth_m is not None:\n    valid_depth = np.isfinite(depth_m) &amp; (depth_m &gt; 0)\n    max_depth = np.percentile(depth_m[valid_depth], 95)\n    depth_display = np.clip(depth_m, 0, max_depth)\n    depth_display[~valid_depth] = max_depth\n\n    im2 = axes[2].imshow(depth_display, cmap='turbo_r', vmin=0, vmax=max_depth)\n    axes[2].set_title('Depth (meters)')\n    axes[2].axis('off')\n    plt.colorbar(im2, ax=axes[2], fraction=0.046)\n\nplt.tight_layout()\nplt.savefig('depth_comparison.png', dpi=150)\nplt.show()\n</code></pre>"},{"location":"examples/#parameter-tuning","title":"Parameter Tuning","text":""},{"location":"examples/#finding-optimal-num_disp","title":"Finding Optimal num_disp","text":"<pre><code>import depthlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Test different num_disp values\nnum_disp_values = [64, 128, 192, 256]\nresults = []\n\nfor nd in num_disp_values:\n    estimator = depthlib.StereoDepthEstimator(\n        left_source='./left.png',\n        right_source='./right.png',\n        downscale_factor=0.5\n    )\n    estimator.configure_sgbm(\n        num_disp=nd,\n        focal_length=1000,\n        baseline=0.1\n    )\n\n    import time\n    start = time.time()\n    disparity_px, depth_m = estimator.estimate_depth()\n    elapsed = (time.time() - start) * 1000\n\n    valid = disparity_px &gt; 0\n    coverage = valid.sum() / valid.size * 100\n\n    results.append({\n        'num_disp': nd,\n        'time_ms': elapsed,\n        'coverage': coverage\n    })\n\n    print(f\"num_disp={nd}: {elapsed:.1f}ms, coverage={coverage:.1f}%\")\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nnds = [r['num_disp'] for r in results]\nax1.bar(nds, [r['time_ms'] for r in results])\nax1.set_xlabel('num_disp')\nax1.set_ylabel('Time (ms)')\nax1.set_title('Processing Time')\n\nax2.bar(nds, [r['coverage'] for r in results])\nax2.set_xlabel('num_disp')\nax2.set_ylabel('Valid pixels (%)')\nax2.set_title('Disparity Coverage')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/#error-handling","title":"Error Handling","text":""},{"location":"examples/#robust-stereo-estimation","title":"Robust Stereo Estimation","text":"<pre><code>import depthlib\nimport numpy as np\n\ndef estimate_depth_safe(left_path, right_path, **sgbm_params):\n    \"\"\"Estimate depth with error handling.\"\"\"\n    try:\n        estimator = depthlib.StereoDepthEstimator(\n            left_source=left_path,\n            right_source=right_path,\n            downscale_factor=0.5\n        )\n        estimator.configure_sgbm(**sgbm_params)\n\n        disparity_px, depth_m = estimator.estimate_depth()\n\n        # Validate results\n        valid_disp = disparity_px &gt; 0\n        if valid_disp.sum() &lt; 0.1 * valid_disp.size:\n            print(\"Warning: Less than 10% valid disparities\")\n            return None, None\n\n        return disparity_px, depth_m\n\n    except FileNotFoundError as e:\n        print(f\"Image not found: {e}\")\n        return None, None\n    except ValueError as e:\n        print(f\"Invalid parameter: {e}\")\n        return None, None\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return None, None\n\n# Usage\ndisparity, depth = estimate_depth_safe(\n    './left.png',\n    './right.png',\n    num_disp=128,\n    focal_length=1000,\n    baseline=0.1\n)\n\nif depth is not None:\n    print(\"Success!\")\n</code></pre>"},{"location":"examples/#integration-example","title":"Integration Example","text":""},{"location":"examples/#obstacle-detection","title":"Obstacle Detection","text":"<pre><code>from depthlib import StereoDepthEstimatorVideo\nimport numpy as np\n\nDANGER_DISTANCE = 1.0  # meters\nWARNING_DISTANCE = 3.0  # meters\n\nestimator = StereoDepthEstimatorVideo(\n    left_source=0,\n    right_source=1,\n    visualize_live=True,\n    fast_mode=True\n)\n\nestimator.configure_sgbm(\n    num_disp=128,\n    focal_length=679.01,\n    baseline=0.5725\n)\n\nprint(\"Obstacle Detection Running... (ESC to exit)\")\n\nfor depth_m in estimator.estimate_depth():\n    valid = np.isfinite(depth_m) &amp; (depth_m &gt; 0)\n\n    if valid.any():\n        closest = depth_m[valid].min()\n\n        if closest &lt; DANGER_DISTANCE:\n            print(f\"\u26a0\ufe0f  DANGER! Object at {closest:.2f}m\")\n        elif closest &lt; WARNING_DISTANCE:\n            print(f\"\u26a1 Warning: Object at {closest:.2f}m\")\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This guide will help you set up Depthlib and run your first depth estimation.</p>"},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#quick-install-recommended","title":"Quick Install (Recommended)","text":"<pre><code>pip install depthlib\n</code></pre> <p>For GPU acceleration, also install PyTorch with CUDA support:</p> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n</code></pre> <p>Finding Your CUDA Version</p> <p>Visit pytorch.org/get-started/locally to select the correct CUDA version for your system.</p>"},{"location":"getting_started/#development-installation","title":"Development Installation","text":"<p>If you want to contribute or modify the library, follow these steps:</p>"},{"location":"getting_started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/your-username/depthlib.git\ncd depthlib\n</code></pre>"},{"location":"getting_started/#2-create-a-virtual-environment","title":"2. Create a Virtual Environment","text":"<pre><code>python -m venv venv\n</code></pre>"},{"location":"getting_started/#3-activate-the-virtual-environment","title":"3. Activate the Virtual Environment","text":"<p>=== \"Windows\"</p> <pre><code>```bash\nvenv\\Scripts\\activate\n```\n</code></pre> <p>=== \"macOS/Linux\"</p> <pre><code>```bash\nsource venv/bin/activate\n```\n</code></pre>"},{"location":"getting_started/#4-install-dependencies","title":"4. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting_started/#5-install-in-editable-mode","title":"5. Install in Editable Mode","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"getting_started/#6-install-pytorch","title":"6. Install PyTorch","text":"<p>Choose the appropriate version based on your hardware:</p> <p>CPU-only: <pre><code>pip install torch\n</code></pre></p> <p>CUDA (GPU): <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n</code></pre></p>"},{"location":"getting_started/#project-structure","title":"Project Structure","text":"<pre><code>depthlib/\n\u251c\u2500\u2500 __init__.py                    # Main exports\n\u251c\u2500\u2500 StereoDepthEstimator.py        # Stereo image depth\n\u251c\u2500\u2500 StereoDepthEstimatorVideo.py   # Stereo video depth\n\u251c\u2500\u2500 MonocularDepthEstimator.py     # AI-based monocular depth\n\u251c\u2500\u2500 stereo_core.py                 # SGBM implementation\n\u251c\u2500\u2500 visualizations.py              # Display utilities\n\u251c\u2500\u2500 input.py                       # Image/video loading\n\u251c\u2500\u2500 rectify.py                     # Stereo rectification\n\u2514\u2500\u2500 postprocess.py                 # Disparity filtering\n</code></pre>"},{"location":"getting_started/#your-first-depth-estimation","title":"Your First Depth Estimation","text":""},{"location":"getting_started/#stereo-images","title":"Stereo Images","text":"<pre><code>import depthlib\n\n# Initialize estimator with stereo pair\nestimator = depthlib.StereoDepthEstimator(\n    left_source='./assets/stereo_pairs/im0.png',\n    right_source='./assets/stereo_pairs/im1.png',\n    downscale_factor=0.5\n)\n\n# Configure SGBM with camera parameters\nestimator.configure_sgbm(\n    num_disp=280,\n    focal_length=3997.684,      # pixels\n    baseline=0.193,             # meters\n    doffs=131.111               # disparity offset\n)\n\n# Estimate depth\ndisparity_px, depth_m = estimator.estimate_depth()\n\n# Visualize results\nestimator.visualize_results()\n</code></pre>"},{"location":"getting_started/#live-video-stream","title":"Live Video Stream","text":"<pre><code>from depthlib import StereoDepthEstimatorVideo\n\n# Initialize with video sources (files or camera indices)\nestimator = StereoDepthEstimatorVideo(\n    left_source='./assets/left.mp4',\n    right_source='./assets/right.mp4',\n    downscale_factor=0.7,\n    visualize_live=True,\n    target_fps=30\n)\n\n# Configure SGBM\nestimator.configure_sgbm(\n    num_disp=128,\n    focal_length=679.01,\n    baseline=0.5725\n)\n\n# Process video frames\nfor depth_m in estimator.estimate_depth():\n    # depth_m contains the depth map for each frame\n    pass  # Press ESC to exit\n</code></pre>"},{"location":"getting_started/#monocular-depth-single-image","title":"Monocular Depth (Single Image)","text":"<pre><code>import depthlib\n\n# Initialize with pre-trained model\nmodel_path = \"models/hub/models--depth-anything--Depth-Anything-V2-Base-hf/snapshots/...\"\nestimator = depthlib.MonocularDepthEstimator(\n    model_path=model_path,\n    device='cuda',  # or 'cpu'\n    downscale_factor=0.5\n)\n\n# Estimate depth from single image\ndepth_map = estimator.estimate_depth(image_path='./assets/image.png')\n\n# Visualize\nestimator.visualize_depth()\n</code></pre>"},{"location":"getting_started/#understanding-camera-parameters","title":"Understanding Camera Parameters","text":"<p>For accurate metric depth from stereo vision, you need calibration parameters:</p> Parameter Description Units <code>focal_length</code> Camera focal length pixels <code>baseline</code> Distance between camera centers meters <code>doffs</code> Disparity offset (cx_right - cx_left) pixels <code>num_disp</code> Maximum disparity range to search pixels <p>Depth Formula</p> <p>Depth is calculated as: $Z = \\frac{f \\cdot B}{d + d_{offs}}$</p> <p>Where $f$ is focal length, $B$ is baseline, and $d$ is disparity.</p>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>StereoDepthEstimator API - Complete reference for image-based depth</li> <li>StereoDepthEstimatorVideo API - Video/live camera depth</li> <li>SGBM Configuration - Fine-tune matching parameters</li> <li>Examples - More code examples</li> </ul>"},{"location":"mono_depth/","title":"Monocular Depth Estimation","text":"<p>The <code>MonocularDepthEstimator</code> class estimates depth from a single RGB image using a pre-trained deep learning model (Depth Anything V2).</p> <p>No Stereo Required</p> <p>Unlike stereo depth estimation, monocular depth works with a single image. However, the depth values are relative (not metric) and depend on model generalization.</p>"},{"location":"mono_depth/#class-monoculardepthestimator","title":"Class: MonocularDepthEstimator","text":"<pre><code>from depthlib import MonocularDepthEstimator\n</code></pre>"},{"location":"mono_depth/#constructor","title":"Constructor","text":"<pre><code>MonocularDepthEstimator(\n    model_path: str,\n    device: Literal['cpu', 'cuda'] = 'cpu',\n    downscale_factor: float = 1.0\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model_path</code> <code>str</code> Required Path to the pre-trained model directory <code>device</code> <code>str</code> <code>'cpu'</code> Computation device: <code>'cpu'</code> or <code>'cuda'</code> (GPU) <code>downscale_factor</code> <code>float</code> <code>1.0</code> Scale factor for image resizing (0 &lt; factor \u2264 1.0) <p>Requirements</p> <ul> <li>PyTorch must be installed</li> <li>For <code>device='cuda'</code>, PyTorch CUDA version is required</li> <li>Model files must be downloaded separately</li> </ul> <p>Example:</p> <pre><code>model_path = \"models/hub/models--depth-anything--Depth-Anything-V2-Base-hf/snapshots/b1958afc...\"\n\nestimator = MonocularDepthEstimator(\n    model_path=model_path,\n    device='cuda',  # Use GPU\n    downscale_factor=0.5\n)\n</code></pre>"},{"location":"mono_depth/#methods","title":"Methods","text":""},{"location":"mono_depth/#estimate_depth","title":"estimate_depth()","text":"<p>Estimate relative depth from a single image.</p> <pre><code>estimate_depth(image_path: str) -&gt; np.ndarray\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>image_path</code> <code>str</code> Path to the input RGB image <p>Returns:</p> Return Value Type Description <code>depth_map</code> <code>np.ndarray</code> Relative depth map (higher values = closer) <p>Depth Values</p> <p>The returned depth values are inverted for visualization purposes: - Higher values = closer objects - Lower values = farther objects</p> <p>Values are relative, not metric (not in meters).</p> <p>Example:</p> <pre><code>depth_map = estimator.estimate_depth(image_path='./image.png')\n\nprint(f\"Depth map shape: {depth_map.shape}\")\nprint(f\"Value range: {depth_map.min():.2f} - {depth_map.max():.2f}\")\n</code></pre>"},{"location":"mono_depth/#visualize_depth","title":"visualize_depth()","text":"<p>Display the estimated depth map using Matplotlib.</p> <pre><code>visualize_depth() -&gt; None\n</code></pre> <p>Prerequisites</p> <p>You must call <code>estimate_depth()</code> before calling <code>visualize_depth()</code>, otherwise a <code>RuntimeError</code> will be raised.</p> <p>Example:</p> <pre><code>estimator.estimate_depth('./image.png')\nestimator.visualize_depth()  # Opens matplotlib window\n</code></pre>"},{"location":"mono_depth/#load_model","title":"load_model()","text":"<p>Load or reload the pre-trained model.</p> <pre><code>load_model() -&gt; None\n</code></pre> <p>Automatic Loading</p> <p>This method is called automatically during initialization. You only need to call it manually if you want to reload the model.</p>"},{"location":"mono_depth/#warmup","title":"warmup()","text":"<p>Perform a warmup inference to optimize performance.</p> <pre><code>warmup() -&gt; None\n</code></pre> <p>Automatic Warmup</p> <p>This method is called automatically during initialization.</p>"},{"location":"mono_depth/#model-setup","title":"Model Setup","text":""},{"location":"mono_depth/#supported-models","title":"Supported Models","text":"<p>The library supports Depth Anything V2 models from Hugging Face:</p> Model Size Quality Speed <code>Depth-Anything-V2-Small-hf</code> ~98MB Good Fast <code>Depth-Anything-V2-Base-hf</code> ~390MB Better Medium <code>Depth-Anything-V2-Large-hf</code> ~1.4GB Best Slow"},{"location":"mono_depth/#download-model","title":"Download Model","text":"<p>Download the model from Hugging Face Hub:</p> <pre><code># Using git-lfs\ngit lfs install\ngit clone https://huggingface.co/depth-anything/Depth-Anything-V2-Base-hf\n\n# Or using huggingface_hub\npip install huggingface_hub\nhuggingface-cli download depth-anything/Depth-Anything-V2-Base-hf\n</code></pre>"},{"location":"mono_depth/#model-directory-structure","title":"Model Directory Structure","text":"<pre><code>models/hub/models--depth-anything--Depth-Anything-V2-Base-hf/\n\u2514\u2500\u2500 snapshots/\n    \u2514\u2500\u2500 b1958afc87fb45a9e3746cb387596094de553ed8/\n        \u251c\u2500\u2500 config.json\n        \u251c\u2500\u2500 model.safetensors\n        \u2514\u2500\u2500 preprocessor_config.json\n</code></pre>"},{"location":"mono_depth/#complete-example","title":"Complete Example","text":"<pre><code>import depthlib\nimport time\n\n# Model path\nmodel_path = \"models/hub/models--depth-anything--Depth-Anything-V2-Base-hf/snapshots/b1958afc87fb45a9e3746cb387596094de553ed8\"\n\n# Initialize estimator\nestimator = depthlib.MonocularDepthEstimator(\n    model_path=model_path,\n    device='cuda',      # Use GPU for faster inference\n    downscale_factor=0.5\n)\n\n# Estimate depth\nimage_path = './assets/image.png'\n\nstart_time = time.time()\ndepth_map = estimator.estimate_depth(image_path=image_path)\nlatency_ms = (time.time() - start_time) * 1000\n\nprint(f\"Depth estimation completed in {latency_ms:.2f} ms\")\nprint(f\"Depth map shape: {depth_map.shape}\")\nprint(f\"Value range: {depth_map.min():.2f} - {depth_map.max():.2f}\")\n\n# Visualize\nestimator.visualize_depth()\n</code></pre>"},{"location":"mono_depth/#error-handling","title":"Error Handling","text":""},{"location":"mono_depth/#common-errors","title":"Common Errors","text":"<p>PyTorch Not Installed: <pre><code># Raises ImportError\nImportError: PyTorch is not installed. Please install the cpu or cuda version of PyTorch.\n</code></pre></p> <p>CUDA Not Available: <pre><code># Raises EnvironmentError when device='cuda' but CUDA is not available\nEnvironmentError: CUDA is not available. Please check if you have torch cuda version or use device='cpu'.\n</code></pre></p> <p>Model Not Found: <pre><code># Raises Exception when model_path is invalid\nException: Error loading model: ...\n</code></pre></p> <p>No Model Path: <pre><code># Raises ValueError\nValueError: Model path must be provided.\n</code></pre></p>"},{"location":"mono_depth/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use GPU: Set <code>device='cuda'</code> for significantly faster inference</li> <li>Downscale Images: Use <code>downscale_factor=0.5</code> or lower for faster processing</li> <li>Batch Processing: The model performs a warmup on first run; subsequent calls are faster</li> </ol>"},{"location":"mono_depth/#monocular-vs-stereo-depth","title":"Monocular vs Stereo Depth","text":"Feature Monocular Stereo Input Single image Image pair Output Relative depth Metric depth (meters) Calibration Not required Required Accuracy Depends on scene Geometric precision Speed Model-dependent Fast (CPU-based)"},{"location":"mono_depth/#see-also","title":"See Also","text":"<ul> <li>Stereo Depth Images - Metric depth from stereo pairs</li> <li>Stereo Depth Video - Real-time video depth</li> <li>Visualization Utilities - Custom visualization options</li> </ul>"},{"location":"sgbm_config/","title":"SGBM Configuration Guide","text":"<p>This guide explains how to tune Semi-Global Block Matching (SGBM) parameters for optimal depth estimation results.</p>"},{"location":"sgbm_config/#overview","title":"Overview","text":"<p>SGBM (Semi-Global Block Matching) is the core algorithm used for stereo matching. Proper configuration is essential for accurate depth maps.</p>"},{"location":"sgbm_config/#basic-formula","title":"Basic Formula","text":"<p>Depth is calculated from disparity using:</p> <p>$$Z = \\frac{f \\cdot B}{d + d_{offs}}$$</p> <p>Where:</p> <ul> <li>$Z$ = Depth (meters)</li> <li>$f$ = Focal length (pixels)</li> <li>$B$ = Baseline (meters)</li> <li>$d$ = Disparity (pixels)</li> <li>$d_{offs}$ = Disparity offset</li> </ul>"},{"location":"sgbm_config/#camera-parameters","title":"Camera Parameters","text":"<p>These parameters define your stereo camera setup and are required for metric depth.</p>"},{"location":"sgbm_config/#focal_length","title":"focal_length","text":"<p>Camera focal length in pixels.</p> Property Value Type <code>float</code> Default <code>None</code> Units pixels <p>How to obtain:</p> <pre><code># From camera matrix K\nK = [[fx, 0, cx],\n     [0, fy, cy],\n     [0, 0,  1]]\nfocal_length = fx  # or fy (usually similar)\n</code></pre>"},{"location":"sgbm_config/#baseline","title":"baseline","text":"<p>Distance between the two camera centers in meters.</p> Property Value Type <code>float</code> Default <code>None</code> Units meters <p>Example:</p> <pre><code>baseline_mm = 572.5  # millimeters\nbaseline = baseline_mm / 1000.0  # Convert to meters: 0.5725\n</code></pre>"},{"location":"sgbm_config/#doffs","title":"doffs","text":"<p>Disparity offset (difference in principal points between cameras).</p> Property Value Type <code>float</code> Default <code>0.0</code> Units pixels <p>Formula: <code>doffs = cx_right - cx_left</code></p>"},{"location":"sgbm_config/#max_depth","title":"max_depth","text":"<p>Maximum depth value to clamp results.</p> Property Value Type <code>float</code> or <code>None</code> Default <code>None</code> Units meters <p>Example:</p> <pre><code>estimator.configure_sgbm(\n    max_depth=50.0  # Clamp depth to 50 meters\n)\n</code></pre>"},{"location":"sgbm_config/#disparity-parameters","title":"Disparity Parameters","text":""},{"location":"sgbm_config/#num_disp","title":"num_disp","text":"<p>Number of disparities to search (disparity range).</p> Property Value Type <code>int</code> Default <code>128</code> Constraint Must be divisible by 16 <p>Choosing num_disp</p> <ul> <li>Larger values \u2192 Detect closer objects, but slower</li> <li>Smaller values \u2192 Faster, but miss close objects</li> </ul> <p>Rule of thumb: <code>num_disp \u2248 image_width / 4</code> to <code>image_width / 8</code></p> <p>Common values: <code>64</code>, <code>128</code>, <code>192</code>, <code>256</code>, <code>280</code></p>"},{"location":"sgbm_config/#min_disp","title":"min_disp","text":"<p>Minimum disparity value (where to start searching).</p> Property Value Type <code>int</code> Default <code>0</code> <p>Use cases:</p> <ul> <li><code>min_disp=0</code> \u2192 Objects at infinity are valid</li> <li><code>min_disp&gt;0</code> \u2192 Ignore very far objects</li> </ul>"},{"location":"sgbm_config/#block_size","title":"block_size","text":"<p>Size of the matching block (window).</p> Property Value Type <code>int</code> Default <code>5</code> Range <code>1</code> to <code>11</code> (odd numbers) <p>Choosing block_size</p> <ul> <li>Smaller (3-5) \u2192 More detail, but noisier</li> <li>Larger (7-11) \u2192 Smoother, but less detail</li> </ul>"},{"location":"sgbm_config/#quality-parameters","title":"Quality Parameters","text":""},{"location":"sgbm_config/#uniqueness_ratio","title":"uniqueness_ratio","text":"<p>Margin (%) by which the best match must win over the second-best.</p> Property Value Type <code>int</code> Default <code>10</code> Range <code>5</code> to <code>15</code> <p>Effect:</p> <ul> <li>Higher values \u2192 Fewer false matches, but more holes</li> <li>Lower values \u2192 More matches, but potentially more errors</li> </ul>"},{"location":"sgbm_config/#disp12_max_diff","title":"disp12_max_diff","text":"<p>Maximum allowed difference in left-right disparity consistency check.</p> Property Value Type <code>int</code> Default <code>1</code> <p>Effect:</p> <ul> <li><code>1</code> \u2192 Strict consistency (recommended)</li> <li><code>-1</code> \u2192 Disable left-right check</li> </ul>"},{"location":"sgbm_config/#prefilter_cap","title":"prefilter_cap","text":"<p>Truncation value for prefiltered image pixels.</p> Property Value Type <code>int</code> Default <code>31</code> Range <code>1</code> to <code>63</code>"},{"location":"sgbm_config/#speckle-filtering","title":"Speckle Filtering","text":"<p>Removes small isolated disparity regions (noise).</p>"},{"location":"sgbm_config/#speckle_window_size","title":"speckle_window_size","text":"<p>Maximum size of smooth disparity regions to consider as valid.</p> Property Value Type <code>int</code> Default <code>50</code> <p>Effect:</p> <ul> <li>Larger values \u2192 More aggressive noise removal</li> <li><code>0</code> \u2192 Disable speckle filtering</li> </ul>"},{"location":"sgbm_config/#speckle_range","title":"speckle_range","text":"<p>Maximum disparity variation within each connected component.</p> Property Value Type <code>int</code> Default <code>2</code>"},{"location":"sgbm_config/#algorithm-mode","title":"Algorithm Mode","text":""},{"location":"sgbm_config/#sgbm_mode","title":"sgbm_mode","text":"<p>SGBM algorithm variant to use.</p> Property Value Type <code>str</code> Default <code>'sgbm_3way'</code> Options <code>'sgbm'</code>, <code>'hh'</code>, <code>'sgbm_3way'</code>, <code>'hh4'</code> <p>Mode comparison:</p> Mode Quality Speed Description <code>'sgbm'</code> Good Medium Standard 8-direction SGBM <code>'hh'</code> Good Fast Full-scale 2-pass dynamic programming <code>'sgbm_3way'</code> Best Slow 3-direction SGBM (highest quality) <code>'hh4'</code> Good Fast 4-direction variant"},{"location":"sgbm_config/#post-processing","title":"Post-Processing","text":""},{"location":"sgbm_config/#hole_filling","title":"hole_filling","text":"<p>Enable hole filling in the disparity map.</p> Property Value Type <code>bool</code> Default <code>False</code> <p>Effect: Fills invalid disparity regions using inpainting.</p>"},{"location":"sgbm_config/#configuration-examples","title":"Configuration Examples","text":""},{"location":"sgbm_config/#high-quality-slow","title":"High Quality (Slow)","text":"<pre><code>estimator.configure_sgbm(\n    num_disp=256,\n    block_size=5,\n    uniqueness_ratio=10,\n    speckle_window_size=100,\n    speckle_range=2,\n    sgbm_mode='sgbm_3way',\n    hole_filling=True,\n    focal_length=3997.684,\n    baseline=0.193,\n    doffs=131.111\n)\n</code></pre>"},{"location":"sgbm_config/#balanced","title":"Balanced","text":"<pre><code>estimator.configure_sgbm(\n    num_disp=128,\n    block_size=5,\n    uniqueness_ratio=10,\n    speckle_window_size=50,\n    sgbm_mode='sgbm_3way',\n    focal_length=679.01,\n    baseline=0.5725\n)\n</code></pre>"},{"location":"sgbm_config/#fast-real-time","title":"Fast (Real-time)","text":"<pre><code>estimator.configure_sgbm(\n    num_disp=64,\n    block_size=7,\n    uniqueness_ratio=5,\n    speckle_window_size=0,  # Disable\n    sgbm_mode='hh',  # Fastest mode\n    focal_length=679.01,\n    baseline=0.5725\n)\n</code></pre>"},{"location":"sgbm_config/#close-range-scene","title":"Close-Range Scene","text":"<pre><code>estimator.configure_sgbm(\n    num_disp=280,  # Large range for close objects\n    block_size=3,  # Small blocks for detail\n    max_depth=5.0,  # Limit to 5 meters\n    focal_length=3997.684,\n    baseline=0.193\n)\n</code></pre>"},{"location":"sgbm_config/#troubleshooting","title":"Troubleshooting","text":""},{"location":"sgbm_config/#too-many-holes","title":"Too Many Holes","text":"<ul> <li>Decrease <code>uniqueness_ratio</code></li> <li>Enable <code>hole_filling=True</code></li> <li>Increase <code>speckle_window_size</code></li> </ul>"},{"location":"sgbm_config/#noisy-disparity","title":"Noisy Disparity","text":"<ul> <li>Increase <code>block_size</code></li> <li>Increase <code>speckle_window_size</code> and <code>speckle_range</code></li> <li>Use <code>sgbm_mode='sgbm_3way'</code></li> </ul>"},{"location":"sgbm_config/#missing-close-objects","title":"Missing Close Objects","text":"<ul> <li>Increase <code>num_disp</code></li> </ul>"},{"location":"sgbm_config/#depth-values-too-largesmall","title":"Depth Values Too Large/Small","text":"<ul> <li>Verify <code>focal_length</code>, <code>baseline</code>, and <code>doffs</code> values</li> <li>Check that <code>baseline</code> is in meters</li> <li>Verify <code>focal_length</code> is in pixels (not mm)</li> </ul>"},{"location":"sgbm_config/#parameter-reference-table","title":"Parameter Reference Table","text":"Parameter Type Default Description <code>focal_length</code> <code>float</code> <code>None</code> Focal length (pixels) <code>baseline</code> <code>float</code> <code>None</code> Camera baseline (meters) <code>doffs</code> <code>float</code> <code>0.0</code> Disparity offset (pixels) <code>max_depth</code> <code>float</code> <code>None</code> Max depth clamp (meters) <code>num_disp</code> <code>int</code> <code>128</code> Disparity range <code>min_disp</code> <code>int</code> <code>0</code> Minimum disparity <code>block_size</code> <code>int</code> <code>5</code> Matching window size <code>uniqueness_ratio</code> <code>int</code> <code>10</code> Match uniqueness (%) <code>disp12_max_diff</code> <code>int</code> <code>1</code> L-R consistency threshold <code>prefilter_cap</code> <code>int</code> <code>31</code> Prefilter truncation <code>speckle_window_size</code> <code>int</code> <code>50</code> Speckle filter size <code>speckle_range</code> <code>int</code> <code>2</code> Speckle disparity range <code>sgbm_mode</code> <code>str</code> <code>'sgbm_3way'</code> Algorithm variant <code>hole_filling</code> <code>bool</code> <code>False</code> Enable hole filling"},{"location":"stereo_image/","title":"Stereo Depth from Images","text":"<p>The <code>StereoDepthEstimator</code> class computes depth maps from rectified stereo image pairs using Semi-Global Block Matching (SGBM).</p>"},{"location":"stereo_image/#class-stereodepthestimator","title":"Class: StereoDepthEstimator","text":"<pre><code>from depthlib import StereoDepthEstimator\n</code></pre>"},{"location":"stereo_image/#constructor","title":"Constructor","text":"<pre><code>StereoDepthEstimator(\n    left_source: str = None,\n    right_source: str = None,\n    downscale_factor: float = 1.0\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>left_source</code> <code>str</code> <code>None</code> Path to the left stereo image <code>right_source</code> <code>str</code> <code>None</code> Path to the right stereo image <code>downscale_factor</code> <code>float</code> <code>1.0</code> Scale factor for image resizing (0 &lt; factor \u2264 1.0). Lower values = faster processing <p>Downscale Factor</p> <p>The <code>downscale_factor</code> must be between 0 (exclusive) and 1.0 (inclusive). Values outside this range will raise a <code>ValueError</code>.</p> <p>Example:</p> <pre><code>estimator = StereoDepthEstimator(\n    left_source='./left.png',\n    right_source='./right.png',\n    downscale_factor=0.5  # Process at 50% resolution\n)\n</code></pre>"},{"location":"stereo_image/#methods","title":"Methods","text":""},{"location":"stereo_image/#configure_sgbm","title":"configure_sgbm()","text":"<p>Configure the Semi-Global Block Matching algorithm parameters.</p> <pre><code>configure_sgbm(**kwargs) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>min_disp</code> <code>int</code> <code>0</code> Minimum disparity value <code>num_disp</code> <code>int</code> <code>128</code> Number of disparities (must be divisible by 16) <code>block_size</code> <code>int</code> <code>5</code> Block size for matching (odd number, typically 3-11) <code>disp12_max_diff</code> <code>int</code> <code>1</code> Max allowed difference in left-right disparity check <code>prefilter_cap</code> <code>int</code> <code>31</code> Truncation value for prefiltered pixels <code>uniqueness_ratio</code> <code>int</code> <code>10</code> Margin (%) by which best match must win <code>speckle_window_size</code> <code>int</code> <code>50</code> Max size of smooth disparity regions <code>speckle_range</code> <code>int</code> <code>2</code> Max disparity variation within each region <code>sgbm_mode</code> <code>str</code> <code>'sgbm_3way'</code> Algorithm mode: <code>'sgbm'</code>, <code>'hh'</code>, <code>'sgbm_3way'</code>, <code>'hh4'</code> <code>focal_length</code> <code>float</code> <code>None</code> Camera focal length in pixels <code>baseline</code> <code>float</code> <code>None</code> Stereo baseline in meters <code>doffs</code> <code>float</code> <code>0.0</code> Disparity offset (cx_right - cx_left) <code>max_depth</code> <code>float</code> <code>None</code> Maximum depth value to clamp results <code>hole_filling</code> <code>bool</code> <code>False</code> Enable hole filling in disparity map <p>Automatic Scaling</p> <p>Parameters <code>num_disp</code>, <code>focal_length</code>, and <code>doffs</code> are automatically scaled by the <code>downscale_factor</code>.</p> <p>Example:</p> <pre><code>estimator.configure_sgbm(\n    num_disp=280,\n    block_size=5,\n    focal_length=3997.684,\n    baseline=0.193,\n    doffs=131.111,\n    hole_filling=True\n)\n</code></pre>"},{"location":"stereo_image/#get_sgbm_params","title":"get_sgbm_params()","text":"<p>Retrieve the current SGBM configuration.</p> <pre><code>get_sgbm_params() -&gt; Dict[str, Any]\n</code></pre> <p>Returns:</p> <p>A dictionary containing all current SGBM parameters.</p> <p>Example:</p> <pre><code>params = estimator.get_sgbm_params()\nprint(f\"Current num_disp: {params['num_disp']}\")\n</code></pre>"},{"location":"stereo_image/#estimate_depth","title":"estimate_depth()","text":"<p>Compute disparity and depth maps from the loaded stereo pair.</p> <pre><code>estimate_depth() -&gt; Tuple[np.ndarray, Optional[np.ndarray]]\n</code></pre> <p>Returns:</p> Return Value Type Description <code>disparity_px</code> <code>np.ndarray</code> Disparity map in pixels (float32) <code>depth_m</code> <code>np.ndarray</code> or <code>None</code> Depth map in meters (float32), or <code>None</code> if <code>focal_length</code> and <code>baseline</code> are not set <p>Pipeline</p> <p>The estimation pipeline: Raw images \u2192 Rectification \u2192 Disparity computation \u2192 Depth mapping</p> <p>Example:</p> <pre><code>disparity_px, depth_m = estimator.estimate_depth()\n\n# Analyze results\nvalid_mask = disparity_px &gt; 0\nprint(f\"Disparity range: {disparity_px[valid_mask].min():.1f} - {disparity_px[valid_mask].max():.1f} px\")\n\nif depth_m is not None:\n    valid_depth = np.isfinite(depth_m) &amp; (depth_m &gt; 0)\n    print(f\"Depth range: {depth_m[valid_depth].min():.2f} - {depth_m[valid_depth].max():.2f} m\")\n</code></pre>"},{"location":"stereo_image/#visualize_results","title":"visualize_results()","text":"<p>Display the computed disparity and depth maps using Matplotlib.</p> <pre><code>visualize_results() -&gt; None\n</code></pre> <p>Prerequisites</p> <p>You must call <code>estimate_depth()</code> before calling <code>visualize_results()</code>, otherwise a <code>ValueError</code> will be raised.</p> <p>Example:</p> <pre><code>estimator.estimate_depth()\nestimator.visualize_results()  # Opens matplotlib windows\n</code></pre>"},{"location":"stereo_image/#complete-example","title":"Complete Example","text":"<pre><code>import depthlib\nimport time\nimport numpy as np\n\n# Configuration\nleft_image = './assets/stereo_pairs/im0.png'\nright_image = './assets/stereo_pairs/im1.png'\n\n# Camera parameters (from calibration)\nndisp = 280\nfocal_length = 3997.684  # pixels\nbaseline_mm = 193.001    # millimeters\ndoffs = 131.111          # disparity offset\n\n# Initialize estimator\nestimator = depthlib.StereoDepthEstimator(\n    left_source=left_image,\n    right_source=right_image,\n    downscale_factor=0.5\n)\n\n# Configure SGBM\nestimator.configure_sgbm(\n    num_disp=ndisp,\n    focal_length=focal_length,\n    baseline=baseline_mm / 1000.0,  # Convert to meters\n    doffs=doffs,\n)\n\n# Estimate depth with timing\nstart = time.time()\ndisparity_px, depth_m = estimator.estimate_depth()\nlatency_ms = (time.time() - start) * 1000\nprint(f\"Depth estimation completed in {latency_ms:.2f} ms\")\n\n# Print statistics\nvalid_disp = disparity_px &gt; 0\nprint(f\"\\n=== Disparity Statistics ===\")\nprint(f\"Range: {disparity_px[valid_disp].min():.2f} - {disparity_px[valid_disp].max():.2f} pixels\")\nprint(f\"Invalid: {(~valid_disp).sum() / valid_disp.size * 100:.1f}%\")\n\n# Visualize\nestimator.visualize_results()\n</code></pre>"},{"location":"stereo_image/#see-also","title":"See Also","text":"<ul> <li>SGBM Configuration - Detailed parameter tuning guide</li> <li>Stereo Depth Video - Real-time video depth estimation</li> <li>Visualization Utilities - Custom visualization options</li> </ul>"},{"location":"stereo_video/","title":"Stereo Depth from Video","text":"<p>The <code>StereoDepthEstimatorVideo</code> class performs real-time depth estimation from synchronized stereo video streams or live cameras.</p>"},{"location":"stereo_video/#class-stereodepthestimatorvideo","title":"Class: StereoDepthEstimatorVideo","text":"<pre><code>from depthlib import StereoDepthEstimatorVideo\n</code></pre>"},{"location":"stereo_video/#constructor","title":"Constructor","text":"<pre><code>StereoDepthEstimatorVideo(\n    left_source: Union[int, str] = None,\n    right_source: Union[int, str] = None,\n    downscale_factor: float = 1.0,\n    visualize_live: bool = False,\n    saving_path: str = None,\n    fast_mode: bool = False,\n    use_threading: bool = True,\n    target_fps: int = 30,\n    drop_frames: bool = False,\n    visualize_gray: bool = False\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>left_source</code> <code>int</code> or <code>str</code> <code>None</code> Left video path, camera index (e.g., <code>0</code>), or RTSP URL <code>right_source</code> <code>int</code> or <code>str</code> <code>None</code> Right video path, camera index (e.g., <code>1</code>), or RTSP URL <code>downscale_factor</code> <code>float</code> <code>1.0</code> Scale factor for frame resizing (0 &lt; factor \u2264 1.0) <code>visualize_live</code> <code>bool</code> <code>False</code> Show live depth visualization window <code>saving_path</code> <code>str</code> <code>None</code> Path to save output video (not yet implemented) <code>fast_mode</code> <code>bool</code> <code>False</code> Enable fast mode for higher FPS (skips postprocessing) <code>use_threading</code> <code>bool</code> <code>True</code> Use threaded frame capture for better FPS <code>target_fps</code> <code>int</code> <code>30</code> Maximum FPS to process (0 = unlimited) <code>drop_frames</code> <code>bool</code> <code>False</code> Drop frames when processing is slow (recommended for live cameras) <code>visualize_gray</code> <code>bool</code> <code>False</code> Use grayscale visualization instead of color <p>Source Types:</p> Type Example Description Camera Index <code>0</code>, <code>1</code> USB camera device number Video File <code>'./video.mp4'</code> Path to video file RTSP URL <code>'rtsp://...'</code> Network stream URL <p>Example:</p> <pre><code># From video files\nestimator = StereoDepthEstimatorVideo(\n    left_source='./left.mp4',\n    right_source='./right.mp4',\n    downscale_factor=0.7,\n    visualize_live=True,\n    target_fps=30\n)\n\n# From USB cameras\nestimator = StereoDepthEstimatorVideo(\n    left_source=0,\n    right_source=1,\n    downscale_factor=0.5,\n    visualize_live=True,\n    drop_frames=True  # Recommended for live cameras\n)\n</code></pre>"},{"location":"stereo_video/#methods","title":"Methods","text":""},{"location":"stereo_video/#configure_sgbm","title":"configure_sgbm()","text":"<p>Configure the Semi-Global Block Matching algorithm parameters.</p> <pre><code>configure_sgbm(**kwargs) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>min_disp</code> <code>int</code> <code>0</code> Minimum disparity value <code>num_disp</code> <code>int</code> <code>128</code> Number of disparities (must be divisible by 16) <code>block_size</code> <code>int</code> <code>5</code> Block size for matching (odd number, typically 3-11) <code>disp12_max_diff</code> <code>int</code> <code>1</code> Max allowed difference in left-right disparity check <code>prefilter_cap</code> <code>int</code> <code>31</code> Truncation value for prefiltered pixels <code>uniqueness_ratio</code> <code>int</code> <code>10</code> Margin (%) by which best match must win <code>speckle_window_size</code> <code>int</code> <code>50</code> Max size of smooth disparity regions <code>speckle_range</code> <code>int</code> <code>2</code> Max disparity variation within each region <code>sgbm_mode</code> <code>str</code> <code>'sgbm_3way'</code> Algorithm mode: <code>'sgbm'</code>, <code>'hh'</code>, <code>'sgbm_3way'</code>, <code>'hh4'</code> <code>focal_length</code> <code>float</code> <code>None</code> Camera focal length in pixels <code>baseline</code> <code>float</code> <code>None</code> Stereo baseline in meters <code>doffs</code> <code>float</code> <code>0.0</code> Disparity offset (cx_right - cx_left) <code>max_depth</code> <code>float</code> <code>None</code> Maximum depth value to clamp results <code>hole_filling</code> <code>bool</code> <code>False</code> Enable hole filling in disparity map <p>Example:</p> <pre><code>estimator.configure_sgbm(\n    num_disp=128,\n    focal_length=679.01,\n    baseline=0.5725,  # 572.5mm in meters\n    doffs=0,\n    hole_filling=True\n)\n</code></pre>"},{"location":"stereo_video/#estimate_depth","title":"estimate_depth()","text":"<p>Start processing the video stream and yield depth maps for each frame.</p> <pre><code>estimate_depth() -&gt; Generator[np.ndarray, None, None]\n</code></pre> <p>Yields:</p> Value Type Description <code>depth_m</code> <code>np.ndarray</code> Depth map in meters (float32) for each frame <p>Generator Pattern</p> <p>This method returns a generator that yields depth maps frame-by-frame. The generator handles video capture, processing, and cleanup automatically.</p> <p>Keyboard Control</p> <p>Press ESC to stop the video processing loop.</p> <p>Example:</p> <pre><code>for depth_m in estimator.estimate_depth():\n    # Process each depth frame\n    valid_depth = np.isfinite(depth_m) &amp; (depth_m &gt; 0)\n    if valid_depth.any():\n        min_depth = depth_m[valid_depth].min()\n        print(f\"Closest object: {min_depth:.2f} m\")\n</code></pre>"},{"location":"stereo_video/#performance-options","title":"Performance Options","text":""},{"location":"stereo_video/#fast-mode","title":"Fast Mode","text":"<p>Enable <code>fast_mode=True</code> to skip expensive postprocessing steps:</p> <pre><code>estimator = StereoDepthEstimatorVideo(\n    left_source=0,\n    right_source=1,\n    fast_mode=True,  # Faster but noisier depth\n    downscale_factor=0.5\n)\n</code></pre> <p>Trade-offs:</p> Setting Quality Speed <code>fast_mode=False</code> Higher Slower <code>fast_mode=True</code> Lower Faster"},{"location":"stereo_video/#threading","title":"Threading","text":"<p>Threading improves FPS by capturing frames in a separate thread:</p> <pre><code># Threaded capture (default, recommended)\nestimator = StereoDepthEstimatorVideo(\n    left_source=0,\n    right_source=1,\n    use_threading=True\n)\n\n# Non-threaded (useful for debugging)\nestimator = StereoDepthEstimatorVideo(\n    left_source='./left.mp4',\n    right_source='./right.mp4',\n    use_threading=False\n)\n</code></pre>"},{"location":"stereo_video/#frame-dropping","title":"Frame Dropping","text":"<p>For live cameras, enable frame dropping to prevent lag:</p> <pre><code>estimator = StereoDepthEstimatorVideo(\n    left_source=0,\n    right_source=1,\n    drop_frames=True  # Drop old frames when processing is slow\n)\n</code></pre> <p>Video Files</p> <p>Set <code>drop_frames=False</code> when processing video files to ensure all frames are processed.</p>"},{"location":"stereo_video/#complete-examples","title":"Complete Examples","text":""},{"location":"stereo_video/#live-usb-cameras","title":"Live USB Cameras","text":"<pre><code>from depthlib import StereoDepthEstimatorVideo\n\n# Two USB cameras\nestimator = StereoDepthEstimatorVideo(\n    left_source=0,\n    right_source=1,\n    downscale_factor=0.5,\n    visualize_live=True,\n    fast_mode=True,\n    drop_frames=True,\n    target_fps=30\n)\n\nestimator.configure_sgbm(\n    num_disp=128,\n    focal_length=679.01,\n    baseline=0.5725\n)\n\n# Press ESC to exit\nfor depth_m in estimator.estimate_depth():\n    pass\n</code></pre>"},{"location":"stereo_video/#video-files","title":"Video Files","text":"<pre><code>from depthlib import StereoDepthEstimatorVideo\n\nestimator = StereoDepthEstimatorVideo(\n    left_source='./assets/left.mp4',\n    right_source='./assets/right.mp4',\n    downscale_factor=0.7,\n    visualize_live=True,\n    use_threading=True,\n    drop_frames=False,  # Process all frames\n    target_fps=30\n)\n\nestimator.configure_sgbm(\n    num_disp=128,\n    focal_length=679.01,\n    baseline=0.5725,\n    hole_filling=True\n)\n\nframe_count = 0\nfor depth_m in estimator.estimate_depth():\n    frame_count += 1\n\nprint(f\"Processed {frame_count} frames\")\n</code></pre>"},{"location":"stereo_video/#grayscale-visualization","title":"Grayscale Visualization","text":"<pre><code>estimator = StereoDepthEstimatorVideo(\n    left_source='./left.mp4',\n    right_source='./right.mp4',\n    visualize_live=True,\n    visualize_gray=True  # Grayscale depth display\n)\n</code></pre>"},{"location":"stereo_video/#visualization","title":"Visualization","text":"<p>When <code>visualize_live=True</code>, a window displays the depth map with:</p> <ul> <li>FPS counter - Current processing rate</li> <li>Display cap - Maximum displayed depth (50m)</li> <li>Color coding (default):<ul> <li>\ud83d\udd34 Red/Yellow = Close objects</li> <li>\ud83d\udd35 Blue = Far objects</li> </ul> </li> <li>Grayscale mode (<code>visualize_gray=True</code>):<ul> <li>\u2b1c White = Close objects</li> <li>\u2b1b Black = Far objects</li> </ul> </li> </ul>"},{"location":"stereo_video/#see-also","title":"See Also","text":"<ul> <li>SGBM Configuration - Detailed parameter tuning guide</li> <li>Stereo Depth Images - Single image pair depth estimation</li> <li>Visualization Utilities - Custom visualization options</li> </ul>"},{"location":"visualizations/","title":"Visualization Utilities","text":"<p>Depthlib provides built-in visualization functions for displaying stereo pairs, disparity maps, and depth maps.</p>"},{"location":"visualizations/#available-functions","title":"Available Functions","text":"<pre><code>from depthlib import (\n    visualize_stereo_pair,\n    visualize_disparity,\n    visualize_depth,\n    visualize_disparity_and_depth\n)\n</code></pre>"},{"location":"visualizations/#visualize_stereo_pair","title":"visualize_stereo_pair()","text":"<p>Display two images (stereo pair or rectified pair) side by side.</p> <pre><code>visualize_stereo_pair(\n    left_img_rgb: np.ndarray,\n    right_img_rgb: np.ndarray,\n    title_left: str = 'Left Image',\n    title_right: str = 'Right Image'\n) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>left_img_rgb</code> <code>np.ndarray</code> Required Left image (RGB or grayscale) <code>right_img_rgb</code> <code>np.ndarray</code> Required Right image (RGB or grayscale) <code>title_left</code> <code>str</code> <code>'Left Image'</code> Title for left image <code>title_right</code> <code>str</code> <code>'Right Image'</code> Title for right image <p>Example:</p> <pre><code>import cv2\nfrom depthlib import visualize_stereo_pair\n\nleft = cv2.imread('left.png')\nright = cv2.imread('right.png')\n\n# Convert BGR to RGB for display\nleft_rgb = cv2.cvtColor(left, cv2.COLOR_BGR2RGB)\nright_rgb = cv2.cvtColor(right, cv2.COLOR_BGR2RGB)\n\nvisualize_stereo_pair(left_rgb, right_rgb)\n</code></pre>"},{"location":"visualizations/#visualize_disparity","title":"visualize_disparity()","text":"<p>Visualize a disparity map with proper colormap.</p> <pre><code>visualize_disparity(\n    disparity_px: np.ndarray,\n    title: str = 'Disparity Map',\n    cmap: str = 'jet',\n    vmin: float = None,\n    vmax: float = None\n) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>disparity_px</code> <code>np.ndarray</code> Required Raw disparity map in pixels <code>title</code> <code>str</code> <code>'Disparity Map'</code> Plot title <code>cmap</code> <code>str</code> <code>'jet'</code> Matplotlib colormap <code>vmin</code> <code>float</code> <code>None</code> Min value for color scaling (auto if None) <code>vmax</code> <code>float</code> <code>None</code> Max value for color scaling (auto if None) <p>Available colormaps:</p> <ul> <li><code>'jet'</code> - Rainbow (default)</li> <li><code>'turbo'</code> - Improved rainbow</li> <li><code>'plasma'</code> - Purple to yellow</li> <li><code>'viridis'</code> - Green to yellow</li> <li><code>'magma'</code> - Black to white through purple</li> </ul> <p>Example:</p> <pre><code>from depthlib import visualize_disparity\n\n# After computing disparity\nvisualize_disparity(disparity_px, title='My Disparity', cmap='turbo')\n\n# With custom range\nvisualize_disparity(disparity_px, vmin=10, vmax=200)\n</code></pre>"},{"location":"visualizations/#visualize_depth","title":"visualize_depth()","text":"<p>Visualize a depth map with proper colormap.</p> <pre><code>visualize_depth(\n    depth_m: np.ndarray,\n    title: str = 'Depth Map',\n    cmap: str = 'turbo_r',\n    max_depth: float = None,\n    show_invalid: bool = True,\n    show_meter: bool = True\n) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>depth_m</code> <code>np.ndarray</code> Required Depth map in meters <code>title</code> <code>str</code> <code>'Depth Map'</code> Plot title <code>cmap</code> <code>str</code> <code>'turbo_r'</code> Matplotlib colormap (reversed for depth) <code>max_depth</code> <code>float</code> <code>None</code> Max depth to display (auto if None) <code>show_invalid</code> <code>bool</code> <code>True</code> Show invalid regions (inf/far) in black <code>show_meter</code> <code>bool</code> <code>True</code> Display depth in meters on colorbar <p>Color Interpretation</p> <p>With <code>'turbo_r'</code> colormap:</p> <ul> <li>\ud83d\udd34 Red/Yellow = Close objects</li> <li>\ud83d\udd35 Blue = Far objects</li> </ul> <p>Example:</p> <pre><code>from depthlib import visualize_depth\n\n# Basic usage\nvisualize_depth(depth_m)\n\n# Custom settings\nvisualize_depth(\n    depth_m,\n    title='Scene Depth',\n    max_depth=10.0,  # Limit display to 10 meters\n    cmap='plasma'\n)\n</code></pre>"},{"location":"visualizations/#visualize_disparity_and_depth","title":"visualize_disparity_and_depth()","text":"<p>Visualize disparity and depth side by side with optional reference image.</p> <pre><code>visualize_disparity_and_depth(\n    disparity_px: np.ndarray,\n    depth_m: np.ndarray,\n    left_img: np.ndarray = None\n) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>disparity_px</code> <code>np.ndarray</code> Required Disparity map in pixels <code>depth_m</code> <code>np.ndarray</code> Required Depth map in meters <code>left_img</code> <code>np.ndarray</code> <code>None</code> Optional reference image <p>Example:</p> <pre><code>from depthlib import visualize_disparity_and_depth\n\n# Without reference image\nvisualize_disparity_and_depth(disparity_px, depth_m)\n\n# With reference image\nvisualize_disparity_and_depth(disparity_px, depth_m, left_img=left_image)\n</code></pre>"},{"location":"visualizations/#live-visualization","title":"Live Visualization","text":"<p>For real-time video depth estimation, the library provides OpenCV-based visualization:</p>"},{"location":"visualizations/#visualize_depth_live","title":"visualize_depth_live()","text":"<p>Display depth map in a live window with FPS overlay (color mode).</p> <pre><code>visualize_depth_live(depth_m: np.ndarray, fps: float) -&gt; None\n</code></pre> <p>Features:</p> <ul> <li>Real-time display using OpenCV</li> <li>FPS counter overlay</li> <li>Display cap indicator (50m default)</li> <li>Turbo colormap (red = close, blue = far)</li> </ul>"},{"location":"visualizations/#visualize_depth_live_gray","title":"visualize_depth_live_gray()","text":"<p>Display depth map in a live window with FPS overlay (grayscale mode).</p> <pre><code>visualize_depth_live_gray(depth_m: np.ndarray, fps: float) -&gt; None\n</code></pre> <p>Features:</p> <ul> <li>Grayscale display (white = close, black = far)</li> <li>FPS counter overlay</li> <li>Display cap indicator</li> </ul>"},{"location":"visualizations/#usage-with-estimators","title":"Usage with Estimators","text":""},{"location":"visualizations/#stereodepthestimator","title":"StereoDepthEstimator","text":"<pre><code>import depthlib\n\nestimator = depthlib.StereoDepthEstimator(\n    left_source='left.png',\n    right_source='right.png'\n)\nestimator.configure_sgbm(num_disp=128, focal_length=1000, baseline=0.1)\n\n# Estimate depth\ndisparity_px, depth_m = estimator.estimate_depth()\n\n# Use built-in visualization\nestimator.visualize_results()\n\n# Or use custom visualization\ndepthlib.visualize_depth(depth_m, title='Custom Title', max_depth=20.0)\n</code></pre>"},{"location":"visualizations/#stereodepthestimatorvideo","title":"StereoDepthEstimatorVideo","text":"<pre><code>from depthlib import StereoDepthEstimatorVideo\n\nestimator = StereoDepthEstimatorVideo(\n    left_source='left.mp4',\n    right_source='right.mp4',\n    visualize_live=True,      # Enable live visualization\n    visualize_gray=False      # Use color (True for grayscale)\n)\n\nfor depth_m in estimator.estimate_depth():\n    # Visualization is handled automatically\n    pass\n</code></pre>"},{"location":"visualizations/#monoculardepthestimator","title":"MonocularDepthEstimator","text":"<pre><code>import depthlib\n\nestimator = depthlib.MonocularDepthEstimator(\n    model_path='path/to/model',\n    device='cuda'\n)\n\ndepth_map = estimator.estimate_depth('image.png')\n\n# Use built-in visualization\nestimator.visualize_depth()\n\n# Or use custom visualization (note: show_meter=False for relative depth)\ndepthlib.visualize_depth(depth_map, show_meter=False)\n</code></pre>"},{"location":"visualizations/#colormap-reference","title":"Colormap Reference","text":"Colormap Best For Description <code>'jet'</code> Disparity Classic rainbow <code>'turbo'</code> Disparity Improved rainbow <code>'turbo_r'</code> Depth Reversed (close=hot) <code>'plasma'</code> General Purple to yellow <code>'viridis'</code> General Perceptually uniform <code>'magma'</code> General Dark to light <code>'gray'</code> Simple Grayscale"},{"location":"visualizations/#tips","title":"Tips","text":"<ol> <li> <p>Depth colormap: Use reversed colormaps (<code>_r</code> suffix) for depth to show close objects as warm colors</p> </li> <li> <p>Auto-scaling: Leave <code>vmin</code>/<code>vmax</code> as <code>None</code> for automatic range detection using percentiles</p> </li> <li> <p>Invalid values: Depth maps may contain <code>inf</code> values for invalid regions; these are handled automatically</p> </li> <li> <p>Live performance: <code>visualize_depth_live()</code> uses OpenCV for faster rendering than Matplotlib</p> </li> </ol>"}]}